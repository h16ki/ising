## Probability theory

***NOTE:*** ここで使っている用語は適当なので（確率論の正確な定義はよくわからない）
正しくないです。気持ちがわかればいい程度のもの。

$X$という事象が起こる確率を$p(X) \in [0, 1]$とする。
このとき$p(X)$は
$$
\begin{aligned}
  \int dx p(x) &= 1, \quad \text{($x$が連続)} \\
  \sum_{x} p(x) &= 1, \quad \text{($x$が有限)}
\end{aligned}
$$
のように規格化されているとする。また$X$を確率変数（事象を実数で表現したもの）という。
確率変数が複数個ある場合は
$$
\begin{aligned}
  \int d\bm{x} p(\bm{x}) &= 1, \\
  \sum_{\bm{x}} p(\bm{x}) &= 1,
\end{aligned}
$$
である。確率変数が互いに独立なときは
$$
  p(\bm{x}) = \prod_{i} p(x_{i}),
$$
である。異なる事象を同時に考えるので同時確率(joint probability)という。
特定の事象のみ興味があるときは、それ以外について積分することでその確率が得られる。
$$
  p(x) = \int dy p(x, y),
  \quad \text{or} \quad
  p(x) = \sum_{y} p(x, y),
$$
これを周辺確率(marginal probability)という。
また複数個の事象のうち、いくつかを固定したときの確率を条件付き確率(conditional probability)といい、
$$
  p(x|y)
$$
と書く。ここで確率変数$x,y$のうち$y$については固定してあり、そのような変数をバーの右側に書く。
このとき
$$
  p(x, y) = p(x | y) p(y) = p(y | x) p(x),
$$
となる。すなわち事象$x,y$が同時に起こる確率は、$x$が起こったときに$y$
が起こる確率（または$y$が起こったときに$x$が起こる確率）と等しいことを意味する。
確率変数が独立であれば明らかに$p(x|y) = p(x)$。

このことから以下のことが直ちにわかる。
$$
  p(x | y) = \frac{p(y | x) p(x)}{p(y)},
$$
これは**ベイズの定理**と呼ばれている。$p(x)$を事前確率(prior probability)、$p(x|y)$を事後確率(posterior probability)という。
これは$p(x)$が全ての$y$について積分したもの、すなわち$y$の起こりうる全ての事象を考慮した確率であることを
意味しており、一方$p(x|y)$は特定の$y$が起こったあとの確率を意味しているからである。

The rule of probability

* sum rule: $p(X) = \sum_{Y} p(X, Y)$
* product rule: $p(X, Y) = p(Y|X) p(X)$

Bayes' theorem

$$
p(Y|X) = \frac{p(X|Y) p(Y)}{p(X)},
p(X) = \sum_{Y} p(X|Y) p(Y)
$$



## Expectations and (co)variances

$$
\gdef\exv{\mathbb{E}}
\gdef\var{\mathrm{var}}
\gdef\cov{\mathrm{cov}}
$$

確率変数$x$を引数に持つ関数$f(x)$があるとする。この関数の期待値を$\exv[f]$と書き、
$$
  \exv[f] \equiv \int dx p(x) f(x),
  \quad\text{or}\quad
  \exv[f] \equiv \sum_{x} p(x) f(x),
$$
と定義する。$f$が多変数関数$f(x,y)$のときは
$$
  \exv[f] = \int d\bm{x} p(\bm{x}) f(\bm{x}),
$$
特定の確率変数だけの期待値を取るときは積分測度を明示して
$$
  \exv_{x}[f] = \int dx p(x, y) f(x, y)
$$
と書く。確率変数のうちいくつかを固定したときの条件付き期待値は
$$
  \exv_{x}[f|y] \equiv \int dx p(x|y) f(x)
$$
となる。

$f(x)$の分散は$f(x)$からその期待値を引いたものの2乗で定義される。
$$
  \var[f] = \exv[ ( f(x) - \exv[f] )^2 ],
$$
または、等価な定義であるが、
$$
  \var[f] = \exv[f^2(x)] - \exv^2[f(x)],
$$
と定義する。これは実際に期待値を計算すればわかる。ただし$f^2(x) = (f(x))^2$である。

2つの確率変数$x,y$の共分散を
$$
  \cov[x,y] = \exv_{x, y}[ { x - \exv[x]} { y - \exv[y] } ] = \exv_{x, y} [xy] - \exv[x] \exv[y],
$$
とする。また$\cov[\bm{x}, \bm{x}] \equiv \cov[\bm{x}]$と書くこともある。

